{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Datos import Datos\n",
    "from EstrategiaParticionado import *\n",
    "from Clasificador import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prueba de la clase Clasificacion, VecinosProximos<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se necesita obtener los datos en un formato correcto. Fijamos k en 5 y el tamaño del datostest en 20% de los datos totales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = 'wdbc.data'\n",
    "\n",
    "dataset = Datos(archivo)\n",
    "k = 5\n",
    "p = 0.2\n",
    "seed = 0\n",
    "\n",
    "datos = dataset.datos\n",
    "# Utilizamos el diccionario para adaptar los datos\n",
    "for i in range(datos.shape[0]):\n",
    "    for j in range(datos.shape[1]):\n",
    "        if dataset.nominalAtributos[j]:\n",
    "            datos[i, j] = dataset.diccionario[j][datos[i, j]]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después, crear las particiones datostrain y datostest. En este caso de prueba utilizaremos validacionSimple con nEjecuciones = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = ValidacionSimple(p, 1)\n",
    "particiones = vs.creaParticiones(len(dataset.datos), seed=seed)\n",
    "\n",
    "datostrain = dataset.extraeDatos(particiones[0].indicesTrain)\n",
    "datostest = dataset.extraeDatos(particiones[0].indicesTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, se entrena el modelo utilizando el conjunto datostrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = ClasificadorVecinosProximos()\n",
    "cl.entrenamiento(datostrain, dataset.nominalAtributos, dataset.diccionario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y, por último se predice la clase para datostest utilizando los porcentajes obtenidos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = cl.clasifica(datostest, dataset.nominalAtributos, dataset.diccionario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos la precisión de nuestra predicción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "print(1-cl.error(datostest, prediccion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos obtenido aproximadamente un 84% de aciertos en este dataset y partición. Vamos a probar ahora los datos \"german.data\" haciendo las mismas operaciones que en el caso anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = 'pima-indians-diabetes.data'\n",
    "\n",
    "dataset = Datos(archivo)\n",
    "p = 0.3\n",
    "seed = 0\n",
    "k = 5\n",
    "\n",
    "datos = dataset.datos\n",
    "for i in range(datos.shape[0]):\n",
    "    for j in range(datos.shape[1]):\n",
    "        if dataset.nominalAtributos[j] or j == datos.shape[1] - 1:\n",
    "            datos[i, j] = dataset.diccionario[j][datos[i, j]]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = ValidacionSimple(p, 1)\n",
    "particiones = vs.creaParticiones(len(dataset.datos), seed=seed)\n",
    "\n",
    "datostrain = dataset.extraeDatos(particiones[0].indicesTrain)\n",
    "datostest = dataset.extraeDatos(particiones[0].indicesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = ClasificadorVecinosProximos()\n",
    "cl.entrenamiento(datostrain, dataset.nominalAtributos, dataset.diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = cl.clasifica(datostest, dataset.nominalAtributos, dataset.diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.991304347826087\n"
     ]
    }
   ],
   "source": [
    "print(1-cl.error(datostest, prediccion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo acierta en aproximadamente 92% de los casos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prueba de la clase Clasificacion, RegresionLogistica<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = 'pima-indians-diabetes.data'\n",
    "\n",
    "dataset = Datos(archivo)\n",
    "p = 0.3\n",
    "seed = 0\n",
    "k = 5\n",
    "\n",
    "datos = dataset.datos\n",
    "for i in range(datos.shape[0]):\n",
    "    for j in range(datos.shape[1]):\n",
    "        if dataset.nominalAtributos[j] or j == datos.shape[1] - 1:\n",
    "            datos[i, j] = dataset.diccionario[j][datos[i, j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = ValidacionSimple(p, 1)\n",
    "particiones = vs.creaParticiones(len(dataset.datos), seed=seed)\n",
    "\n",
    "datostrain = dataset.extraeDatos(particiones[0].indicesTrain)\n",
    "datostest = dataset.extraeDatos(particiones[0].indicesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = ClasificadorRegresionLogistica(alpha=0.001, n_epocas=5000)\n",
    "cl.entrenamiento(datostrain, dataset.nominalAtributos, dataset.diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = cl.clasifica(datostest, dataset.nominalAtributos, dataset.diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7043478260869565\n"
     ]
    }
   ],
   "source": [
    "print(1-cl.error(datostest, prediccion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(prediccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = 'wdbc.data'\n",
    "\n",
    "dataset = Datos(archivo)\n",
    "p = 0.3\n",
    "seed = 0\n",
    "k = 5\n",
    "\n",
    "datos = dataset.datos\n",
    "for i in range(datos.shape[0]):\n",
    "    for j in range(datos.shape[1]):\n",
    "        if dataset.nominalAtributos[j] or j == datos.shape[1] - 1:\n",
    "            datos[i, j] = dataset.diccionario[j][datos[i, j]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = ValidacionSimple(p, 1)\n",
    "particiones = vs.creaParticiones(len(dataset.datos), seed=seed)\n",
    "\n",
    "datostrain = dataset.extraeDatos(particiones[0].indicesTrain)\n",
    "datostest = dataset.extraeDatos(particiones[0].indicesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = ClasificadorRegresionLogistica(alpha=0.001, n_epocas=5000)\n",
    "cl.entrenamiento(datostrain, dataset.nominalAtributos, dataset.diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = cl.clasifica(datostest, dataset.nominalAtributos, dataset.diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "print(1-cl.error(datostest, prediccion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(prediccion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scikit-Learn</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de Scikit-Learn, no hay una función que te permita tratar ciertos datosz como categóricos y ciertos datos del mismo dataset como continuos. Por tanto, utilizaremos MultinomialNB para el caso de datos categóricos (tic-tac-toe.data) y GaussianNB para el caso de que haya algún dato continuo (german.data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero empezamos con las pruebas de validación simple y con tic-tac-toe.data. Transformamos los datos al formato adecuado y realizamos la partición:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = 'pima-indians-diabetes.data'\n",
    "p = 0.3\n",
    "seed = 0\n",
    "\n",
    "dataset = Datos(archivo)\n",
    "\n",
    "datos = dataset.datos\n",
    "# Utilizamos el diccionario para adaptar los datos\n",
    "for i in range(datos.shape[0]):\n",
    "    for j in range(datos.shape[1]):\n",
    "        if dataset.nominalAtributos[j] or j == datos.shape[1] - 1:\n",
    "            datos[i, j] = dataset.diccionario[j][datos[i, j]]  \n",
    "            \n",
    "X = datos[:, :-1]\n",
    "y = datos[:, -1]\n",
    "\n",
    "X = X.astype('int32')\n",
    "y = y.astype('int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=p, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos con la clasificación. Haremos una prueba sin Laplace (alpha=0) y otra con Laplace (alpha=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7792207792207793\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=seed, max_iter=5000).fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = 'wdbc.data'\n",
    "p = 0.3\n",
    "seed = 0\n",
    "\n",
    "dataset = Datos(archivo)\n",
    "\n",
    "datos = dataset.datos\n",
    "for i in range(datos.shape[0]):\n",
    "    for j in range(datos.shape[1]):\n",
    "        if dataset.nominalAtributos[j] or j == datos.shape[1] - 1:\n",
    "            datos[i, j] = dataset.diccionario[j][datos[i, j]]  \n",
    "            \n",
    "X = datos[:, :-1]\n",
    "y = datos[:, -1]\n",
    "\n",
    "X = X.astype('int32')\n",
    "y = y.astype('int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=p, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que no hay diferencia entre haber utilizado Laplace y no haberlo utilizado. Puede que en este caso haya sido así, pero en general es buena idea utilizar Laplace, porque no se sabe como es el dataset, y podría ocurrir que alguna de las probabilidades condicionadas fuesen 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9590643274853801\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=seed, max_iter=5000).fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\ttic-tac-toe scikit\\ttic-tac-toe propio\\tgerman scikit\\t\\tgerman propio')\n",
    "print('VS\\t(', round(1-ac_tic_simple_0, 3), ')\\t\\t(', round(media_vs_a1, 3), round(desv_vs_a1,3), ')\\t\\t(', round(1-ac_german_simple, 3), ')\\t\\t(', round(media_vs_a2, 3), round(desv_vs_a2,3), ')')\n",
    "print('VC\\t(', round(np.mean(1-ac_tic_cruzada_0), 3), round(np.std(1-ac_tic_cruzada_0),3), ')\\t\\t(', round(media_vc_a1, 3), round(desv_vc_a1,3), ')\\t\\t(', round(np.mean(1-ac_german_cruzada), 3), round(np.std(1-ac_german_cruzada),3), ')\\t\\t(', round(media_vc_a2, 3), round(desv_vc_a2,3), ')')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, parece que nuestro estimador comete menos errores que el de Scikit para estos datos y esta semilla. Habría que realizar pruebas mas extensas para medir las calidades y comparar ambos esitimadores adecuadamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluación de hipótesis mediante Análisis ROC</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a realizar este análisis para cada tipo de validación y para cada dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.2\n",
    "seed = 0\n",
    "k = 10\n",
    "\n",
    "def get_ROC_data(conf_data, clases, ocurrencias_clases):\n",
    "    \n",
    "    matriz_conf = np.zeros((2, 2))\n",
    "    matriz_conf[0][0] = ocurrencias_clases[clases[1]]\n",
    "    matriz_conf[0][1] = ocurrencias_clases[clases[0]]\n",
    "    tpr = np.empty(len(datostest))\n",
    "    fpr = np.empty(len(datostest))\n",
    "\n",
    "    for i in range(len(conf_data)):\n",
    "        if conf_data[i][1] == clases[0]: # Dato negativo\n",
    "            matriz_conf[1][1] += 1 #TN\n",
    "            matriz_conf[0][1] -= 1 #FP\n",
    "\n",
    "        else: # Dato positivo\n",
    "            matriz_conf[1][0] += 1 #FN\n",
    "            matriz_conf[0][0] -= 1 #TP\n",
    "\n",
    "\n",
    "        tpr[i] = matriz_conf[0][0]/(matriz_conf[0][0]+matriz_conf[1][0])\n",
    "\n",
    "        fpr[i] = matriz_conf[0][1]/(matriz_conf[0][1]+matriz_conf[1][1])\n",
    "    \n",
    "    return tpr, fpr\n",
    "\n",
    "def get_matriz_conf(datostest, prediccion, clases):\n",
    "    matriz_conf = np.zeros((2, 2))\n",
    "    \n",
    "    for i in range(len(datostest)):\n",
    "        if prediccion[i] == clases[0]:\n",
    "            if datostest[i][-1] == clases[0]: # TN\n",
    "                matriz_conf[1][1] += 1\n",
    "            else: # FN \n",
    "                matriz_conf[1][0] += 1\n",
    "        else:\n",
    "            if datostest[i][-1] == clases[0]: # FP\n",
    "                matriz_conf[0][1] += 1\n",
    "            else: # TP\n",
    "                matriz_conf[0][0] += 1\n",
    "    \n",
    "    return matriz_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>tic-tac-toe.data y validación simple</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = 'tic-tac-toe.data'\n",
    "\n",
    "dataset = Datos(archivo)\n",
    "\n",
    "datos = dataset.datos\n",
    "# Utilizamos el diccionario para adaptar los datos\n",
    "for i in range(datos.shape[0]):\n",
    "    for j in range(datos.shape[1]):\n",
    "        if dataset.nominalAtributos[j] or j==datos.shape[1]-1:\n",
    "            datos[i, j] = dataset.diccionario[j][datos[i, j]]\n",
    "\n",
    "\n",
    "vs = ValidacionSimple(p, 1)\n",
    "particiones = vs.creaParticiones(len(dataset.datos), seed=seed)\n",
    "\n",
    "datostrain = dataset.extraeDatos(particiones[0].indicesTrain)\n",
    "datostest = dataset.extraeDatos(particiones[0].indicesTest)\n",
    "datostest = np.array(datostest)\n",
    "\n",
    "clases, counts = np.unique(datostest[:, -1], return_counts=True)\n",
    "clases = sorted(clases)\n",
    "\n",
    "cl = ClasificadorNaiveBayes()\n",
    "cl.entrenamiento(datostrain, dataset.nominalAtributos, dataset.diccionario)\n",
    "\n",
    "prediccion = cl.clasifica(datostest, dataset.nominalAtributos, dataset.diccionario)\n",
    "\n",
    "conf_data = np.empty((len(prediccion), 2))\n",
    "for i in range(conf_data.shape[0]):\n",
    "    conf_data[i] = [cl.probabilidades[i][1], datostest[i][-1]]\n",
    "\n",
    "conf_data = sorted(conf_data, key=lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos a construir los datos necesarios para la curva ROC y a obtener la matriz de confusión. Como anotación, se asume que la clase negativa es la de menor valor y la positiva la de mayor valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_tic_simple, fpr_tic_simple = get_ROC_data(conf_data, clases, counts)\n",
    "matriz_conf_tic_simple = get_matriz_conf(datostest, prediccion, clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriz de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matriz de confusión tic-tac-toe.data validación simple\")\n",
    "print()\n",
    "print(\"\\t\\t\\t\\tReal\")\n",
    "print(\"\\t\\t  Positivo\\t\\tNegativo\")\n",
    "print(\"Estimado Positivo \", matriz_conf_tic_simple[0][0], \" \\t\\t \", matriz_conf_tic_simple[0][1])\n",
    "print(\"\\t Negativo \", matriz_conf_tic_simple[1][0], \" \\t\\t \", matriz_conf_tic_simple[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr_tic_simple, tpr_tic_simple)\n",
    "plt.title(\"Curva ROC tic-tac-toe.data validación simple\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>tic-tac-toe.data y validación cruzada con k=10</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = 'tic-tac-toe.data'\n",
    "\n",
    "dataset = Datos(archivo)\n",
    "\n",
    "datos = dataset.datos\n",
    "# Utilizamos el diccionario para adaptar los datos\n",
    "for i in range(datos.shape[0]):\n",
    "    for j in range(datos.shape[1]):\n",
    "        if dataset.nominalAtributos[j] or j==datos.shape[1]-1:\n",
    "            datos[i, j] = dataset.diccionario[j][datos[i, j]]\n",
    "\n",
    "\n",
    "vc = ValidacionCruzada(k)\n",
    "\n",
    "particiones = vc.creaParticiones(len(dataset.datos), seed=seed)\n",
    "matrices_conf_tic_cruzada = []\n",
    "tpr_tic_cruzada_lista = []\n",
    "fpr_tic_cruzada_lista = []\n",
    "\n",
    "for particion in particiones:\n",
    "    datostrain = dataset.extraeDatos(particion.indicesTrain)\n",
    "    datostest = dataset.extraeDatos(particion.indicesTest)\n",
    "    datostest = np.array(datostest)\n",
    "\n",
    "    clases, counts = np.unique(datostest[:, -1], return_counts=True)\n",
    "    clases = sorted(clases)\n",
    "\n",
    "\n",
    "    cl = ClasificadorNaiveBayes()\n",
    "    cl.entrenamiento(datostrain, dataset.nominalAtributos, dataset.diccionario)\n",
    "\n",
    "    prediccion = cl.clasifica(datostest, dataset.nominalAtributos, dataset.diccionario)\n",
    "    \n",
    "    conf_data = np.empty((len(prediccion), 2))\n",
    "    for i in range(conf_data.shape[0]):\n",
    "        conf_data[i] = [cl.probabilidades[i][1], datostest[i][-1]]\n",
    "\n",
    "    conf_data = sorted(conf_data, key=lambda x: x[0])\n",
    "    \n",
    "    tpr_tic_cruzada, fpr_tic_cruzada = get_ROC_data(conf_data, clases, counts)\n",
    "    matriz_conf_tic_cruzada = get_matriz_conf(datostest, prediccion, clases)\n",
    "    matrices_conf_tic_cruzada.append(matriz_conf_tic_cruzada)\n",
    "    tpr_tic_cruzada_lista.append(tpr_tic_cruzada)\n",
    "    fpr_tic_cruzada_lista.append(fpr_tic_cruzada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    print(\"Matriz de confusión tic-tac-toe.data validación cruzada fold\", i+1)\n",
    "    print()\n",
    "    print(\"\\t\\t\\t\\tReal\")\n",
    "    print(\"\\t\\t  Positivo\\t\\tNegativo\")\n",
    "    print(\"Estimado Positivo \", matrices_conf_tic_cruzada[i][0][0], \" \\t\\t \", matrices_conf_tic_cruzada[i][0][1])\n",
    "    print(\"\\t Negativo \", matrices_conf_tic_cruzada[i][1][0], \" \\t\\t \", matrices_conf_tic_cruzada[i][1][1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for i in range(k):\n",
    "    plt.plot(fpr_tic_cruzada_lista[i], tpr_tic_cruzada_lista[i], label=\"Fold \" + str(i))\n",
    "    \n",
    "plt.title(\"Curva ROC tic-tac-toe.data validación cruzada\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>german.data y validación simple</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = 'german.data'\n",
    "\n",
    "dataset = Datos(archivo)\n",
    "\n",
    "datos = dataset.datos\n",
    "# Utilizamos el diccionario para adaptar los datos\n",
    "for i in range(datos.shape[0]):\n",
    "    for j in range(datos.shape[1]):\n",
    "        if dataset.nominalAtributos[j] or j==datos.shape[1]-1:\n",
    "            datos[i, j] = dataset.diccionario[j][datos[i, j]]\n",
    "\n",
    "vs = ValidacionSimple(p, 1)\n",
    "particiones = vs.creaParticiones(len(dataset.datos), seed=seed)\n",
    "\n",
    "datostrain = dataset.extraeDatos(particiones[0].indicesTrain)\n",
    "datostest = dataset.extraeDatos(particiones[0].indicesTest)\n",
    "datostest = np.array(datostest)\n",
    "\n",
    "clases, counts = np.unique(datostest[:, -1], return_counts=True)\n",
    "clases = sorted(clases)\n",
    "\n",
    "cl = ClasificadorNaiveBayes()\n",
    "cl.entrenamiento(datostrain, dataset.nominalAtributos, dataset.diccionario)\n",
    "\n",
    "prediccion = cl.clasifica(datostest, dataset.nominalAtributos, dataset.diccionario)\n",
    "\n",
    "conf_data = np.empty((len(prediccion), 2))\n",
    "for i in range(conf_data.shape[0]):\n",
    "    conf_data[i] = [cl.probabilidades[i][1], datostest[i][-1]]\n",
    "\n",
    "conf_data = sorted(conf_data, key=lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_german_simple, fpr_german_simple = get_ROC_data(conf_data, clases, counts)\n",
    "matriz_conf_german_simple = get_matriz_conf(datostest, prediccion, clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matriz de confusión german.data validación simple\")\n",
    "print()\n",
    "print(\"\\t\\t\\t\\tReal\")\n",
    "print(\"\\t\\t  Positivo\\t\\tNegativo\")\n",
    "print(\"Estimado Positivo \", matriz_conf_german_simple[0][0], \" \\t\\t \", matriz_conf_german_simple[0][1])\n",
    "print(\"\\t Negativo \", matriz_conf_german_simple[1][0], \" \\t\\t \", matriz_conf_german_simple[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr_german_simple, tpr_german_simple)\n",
    "plt.title(\"Curva ROC german.data validación simple\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>german.data y validación cruzada con k=10</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = 'german.data'\n",
    "\n",
    "dataset = Datos(archivo)\n",
    "\n",
    "datos = dataset.datos\n",
    "# Utilizamos el diccionario para adaptar los datos\n",
    "for i in range(datos.shape[0]):\n",
    "    for j in range(datos.shape[1]):\n",
    "        if dataset.nominalAtributos[j] or j==datos.shape[1]-1:\n",
    "            datos[i, j] = dataset.diccionario[j][datos[i, j]]\n",
    "\n",
    "\n",
    "vc = ValidacionCruzada(k)\n",
    "\n",
    "particiones = vc.creaParticiones(len(dataset.datos), seed=seed)\n",
    "matrices_conf_german_cruzada = []\n",
    "tpr_german_cruzada_lista = []\n",
    "fpr_german_cruzada_lista = []\n",
    "\n",
    "for particion in particiones:\n",
    "    datostrain = dataset.extraeDatos(particion.indicesTrain)\n",
    "    datostest = dataset.extraeDatos(particion.indicesTest)\n",
    "    datostest = np.array(datostest)\n",
    "\n",
    "    clases, counts = np.unique(datostest[:, -1], return_counts=True)\n",
    "    clases = sorted(clases)\n",
    "\n",
    "    cl = ClasificadorNaiveBayes()\n",
    "    cl.entrenamiento(datostrain, dataset.nominalAtributos, dataset.diccionario)\n",
    "\n",
    "    prediccion = cl.clasifica(datostest, dataset.nominalAtributos, dataset.diccionario)\n",
    "    \n",
    "    conf_data = np.empty((len(prediccion), 2))\n",
    "    for i in range(conf_data.shape[0]):\n",
    "        conf_data[i] = [cl.probabilidades[i][1], datostest[i][-1]]\n",
    "\n",
    "    conf_data = sorted(conf_data, key=lambda x: x[0])\n",
    "    \n",
    "    tpr_german_cruzada, fpr_german_cruzada = get_ROC_data(conf_data, clases, counts)\n",
    "    matriz_conf_german_cruzada = get_matriz_conf(datostest, prediccion, clases)\n",
    "    matrices_conf_german_cruzada.append(matriz_conf_german_cruzada)\n",
    "    tpr_german_cruzada_lista.append(tpr_german_cruzada)\n",
    "    fpr_german_cruzada_lista.append(fpr_german_cruzada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    print(\"Matriz de confusión german.data validación cruzada fold\", i+1)\n",
    "    print()\n",
    "    print(\"\\t\\t\\t\\tReal\")\n",
    "    print(\"\\t\\t  Positivo\\t\\tNegativo\")\n",
    "    print(\"Estimado Positivo \", matrices_conf_german_cruzada[i][0][0], \" \\t\\t \", matrices_conf_german_cruzada[i][0][1])\n",
    "    print(\"\\t Negativo \", matrices_conf_german_cruzada[i][1][0], \" \\t\\t \", matrices_conf_german_cruzada[i][1][1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for i in range(k):\n",
    "    plt.plot(fpr_german_cruzada_lista[i], tpr_german_cruzada_lista[i], label=\"Fold \" + str(i))\n",
    "    \n",
    "plt.title(\"Curva ROC german.data validación cruzada\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, vamos a calcular las áreas bajo las curvas ROC (AUC) para determinar la calidad de cada método:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUC(fpr, tpr):\n",
    "\n",
    "    auc = 0\n",
    "    for i in range(len(tpr)-1):\n",
    "        auc += (fpr[i]-fpr[i+1])*tpr[i]\n",
    "    \n",
    "    auc += fpr[-1]*tpr[-1]\n",
    "        \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_tic_simple = AUC(fpr_tic_simple, tpr_tic_simple)\n",
    "auc_german_simple = AUC(fpr_german_simple, tpr_german_simple)\n",
    "\n",
    "auc_tic_cruzada_lista = []\n",
    "auc_german_cruzada_lista = []\n",
    "\n",
    "for i in range(len(fpr_tic_cruzada_lista)):\n",
    "    auc_tic_cruzada_lista.append(AUC(fpr_tic_cruzada_lista[i], tpr_tic_cruzada_lista[i]))\n",
    "    auc_german_cruzada_lista.append(AUC(fpr_german_cruzada_lista[i], tpr_german_cruzada_lista[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC tic-tac-toe.data VS:\", auc_tic_simple)\n",
    "\n",
    "print()\n",
    "for i in range(len(auc_tic_cruzada_lista)):\n",
    "    print(\"AUC tic-tac-toe.data VC Fold \" + str(i+1) + \":\" , auc_tic_cruzada_lista[i])\n",
    "\n",
    "print()\n",
    "print(\"AUC tic-tac-toe.data VC media: \", np.mean(auc_tic_cruzada_lista))\n",
    "print()\n",
    "print(\"AUC german.data VS:\", auc_german_simple)\n",
    "\n",
    "print()\n",
    "for i in range(len(auc_german_cruzada_lista)):\n",
    "    print(\"AUC german.data VC Fold \" + str(i+1) + \":\" , auc_german_cruzada_lista[i])\n",
    "    \n",
    "print()\n",
    "print(\"AUC german.data VC media: \", np.mean(auc_german_cruzada_lista))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto al dataset tic-tac-toe.data, observamos unos mejores resultados en el caso de la validación cruzada. Sin embargo, en el caso de german.data hay mejores resultados con la validación simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Comentarios respecto a modificaciones en el diseño de clases <h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la clase EstrategiaParticionado, en la función creaParticiones cambiamos los parametros recomendados por un parametro n_datos, ya que en nuestra opinión es el único parámetro necesario para crear los conjuntos de indices. Con esto, deja de ser necesario pasar a la función una instancia de la clase Datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de entrenamiento y clasificación, modificamos el conjunto de datos a un formado adecuado (cambiar los atributos nominales a un entero siguiendo el orden alfabético) utilizando el diccionario fuera de las funciones de la clase Clasificador. Esto nos parece más adecuado en cuanto a la separación las funcionalidades. \n",
    "\n",
    "También, tenemos el criterio de transformar la clase a su valor en el diccionario, sea el atributo nominal o numérico ya que la clase siempre es discreta. \n",
    "Esto permite simplificar el codigo gracias a la posible indexación array[valor1_clase].\n",
    "\n",
    "Asumimos la equivalencia entre nominalAtributos de la clase Datos y el parámetro atributosDiscretos que se pasa a las funciones de la clase Clasificador (con esta suposición resulta coherente transformar los valores de clase, atributo discreto, siempre a un entero entre 0 y n_valores). \n",
    "\n",
    "En la función clasifica añadimos el atributo alpha, el parámetro de la corrección de Laplace. \n",
    "\n",
    "Por último, cambiamos los atributos que se pasan a la función validacion de la clase Clasificador añadiendo todos aquellos atributos necesarios para llamar a las funciones entrenamiento y clasifica. También, eliminamos el parametro clasificador llamando a la función validación como instancia_clasificador.validacion(...)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
